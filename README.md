# Protecting-the-Neural-Networks-against-FGSM-Attack-Using-Machine-Unlearning

**Author:** Amir Hossein Khorasani  
**Supervisors:** Dr. Ali Jahanian & Dr. Maryam Rastgarpour  
**Affiliations:**  
1. Department of Computer Engineering, Islamic Azad University, Saveh Branch, Iran  
2. Faculty of Computer Science and Engineering, Shahid Beheshti University, Tehran, Iran  
**Status:** arXiv preprint (2025)  
**Paper:** [Protecting Neural Networks Against FGSM Attack Using Machine Unlearning](#)

## ðŸ§  Abstract

Machine learning is a powerful tool for building predictive models. However, it is vulnerable to adversarial attacks. Fast
Gradient Sign Method (FGSM) attacks are a common type of adversarial attack that adds small perturbations to input data to trick
a model into misclassifying it. In response to these attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added perturbations. Machine unlearning is a technique that tries
to "forget" specific data points from the training dataset, to improve the robustness of a machine learning model against adversarial
attacks like FGSM.
In this paper, we focus on applying unlearning techniques to the LeNet neural network, a popular architecture for image
classification. We evaluate the efficacy of unlearning FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

Keywords: Machine learning, LeNet neural network, FGSM attacks, Machine unlearning
